import os
import glob
import torch
import numpy as np
import pandas as pd
from tqdm import tqdm
import pickle
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import (
    r2_score,
    classification_report,
    confusion_matrix,
    roc_curve,
    auc,
    mean_squared_error,
    mean_absolute_error,
)
from scipy.stats import pearsonr
from monai.data import DataLoader, PersistentDataset
from monai.transforms import (
    Compose,
    LoadImaged,
    ScaleIntensityRanged,
    EnsureChannelFirstd,
    ToTensord,
    Orientationd,
    Resized,
    MaskIntensityd,
    CropForegroundd,
    Spacingd,
)
import json # æ–°å¢å¯¼å…¥

# --- å¯¼å…¥æ‚¨è‡ªå®šä¹‰çš„æ¨¡å‹ç±» ---
import RegressionModel as model

# --- è¡¥ä¸ä»£ç  (ä¿æŒä¸å˜) ---
orig_torch_load = torch.load
def torch_wrapper(*args, **kwargs):
    kwargs['weights_only'] = False
    return orig_torch_load(*args, **kwargs)
torch.load = torch_wrapper

# --- é…ç½® (ä¸è®­ç»ƒè„šæœ¬ä¿æŒä¸€è‡´) ---
task = 'regression'
image_network_name = 'ResNet'
DATA_ROOT_PATH = "./datasets_corp/datasets"
TEST_CSV_FILE_PATH = "./datasets_corp/æµ‹è¯•é›†.csv"
MODEL_OUTPUT_DIR = f"./model/{task}/{image_network_name}"
REPORT_OUTPUT_DIR = f"./report/{task}/{image_network_name}/plot"
root_dir = "./cache"
persistent_cache = root_dir

BATCH_SIZE = 8
NUM_WORKERS = 4

HU_LOWER_BOUND = -950
HU_UPPER_BOUND = -750
TARGET_SIZE = (128, 128, 128)
TARGET_SPACING = (1.0, 1.0, 1.0)
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ä½¿ç”¨è®¾å¤‡: {DEVICE}")

# --- å®šä¹‰ç‰¹å¾å’Œç›®æ ‡ (ä¸è®­ç»ƒè„šæœ¬ä¿æŒä¸€è‡´) ---
ONE_HOT_INPUT_FEATURES = ['æ€§åˆ«', 'å¸çƒŸå²']
NORMAL_CSV_INPUT_FEATURES = ['å¹´é¾„', 'èº«é«˜', 'ä½“é‡', 'lung_volume_liters', 'LAA910', 'LAA950',
                             'LAA980', 'Mean', 'Median', 'Skewness', 'Kurtosis', 'Variance',
                             'MeanAbsoluteDeviation', 'InterquartileRange', '10Percentile',
                             '90Percentile']
TARGET_COLUMNS = ['FVC', 'FEV1']


# --- è‚ºåŠŸèƒ½è®¡ç®—å…¬å¼ ---
def calculate_fev1_pred_male(age, height):
    return -3.088 + 0.0487 * height - 0.0211 * age

def calculate_fvc_pred_male(age, height):
    return -4.634 + 0.0601 * height - 0.0216 * age

def calculate_fev1_pred_female(age, height):
    return -2.981 + 0.0384 * height - 0.0191 * age

def calculate_fvc_pred_female(age, height):
    return -3.090 + 0.0435 * height - 0.0189 * age

def predict_three_classes(predicted_fev1, predicted_fvc, age, height, sex):
    fev1_fvc_ratio = (predicted_fev1 / predicted_fvc) * 100

    if sex == 'ç”·':
        fev1_pred_calc = calculate_fev1_pred_male(age, height)
        fvc_pred_calc = calculate_fvc_pred_male(age, height)
    else: # sex == 'å¥³'
        fev1_pred_calc = calculate_fev1_pred_female(age, height)
        fvc_pred_calc = calculate_fvc_pred_female(age, height)

    fev1_percent = (predicted_fev1 / fev1_pred_calc) * 100
    fvc_percent = (predicted_fvc / fvc_pred_calc) * 100

    if fev1_fvc_ratio < 70:
        return 'COPD'
    elif fev1_fvc_ratio >= 70 and fev1_percent >= 80 and fvc_percent >= 80:
        return 'NORMAL'
    elif fev1_fvc_ratio >= 70 and (fev1_percent < 80 or fvc_percent < 80):
        return 'PRISm'
    else:
        return 'UNKNOWN' # ç†è®ºä¸Šä¸åº”è¯¥å‡ºç°

# --- æŒ‡æ ‡è®¡ç®—ä¸ç»˜å›¾å‡½æ•° ---
def calculate_regression_metrics(y_true, y_pred, target_columns):
    """è®¡ç®—å¹¶è¿”å›å¤šä¸ªå›å½’æŒ‡æ ‡ã€‚"""
    metrics = {}
    for i, col_name in enumerate(target_columns):
        true_col = y_true[:, i]
        pred_col = y_pred[:, i]

        nan_or_inf_mask = np.isnan(true_col) | np.isinf(true_col) | np.isnan(pred_col) | np.isinf(pred_col)
        valid_indices = ~nan_or_inf_mask

        if not np.any(valid_indices):
            metrics[col_name] = {'r': np.nan, 'r2': np.nan, 'ccc': np.nan, 'rmse': np.nan, 'mse': np.nan, 'mae': np.nan}
            continue

        true_col_valid = true_col[valid_indices]
        pred_col_valid = pred_col[valid_indices]

        try:
            r, _ = pearsonr(true_col_valid, pred_col_valid)
        except ValueError:
            r = np.nan
        r2 = r2_score(true_col_valid, pred_col_valid)

        mean_true = np.mean(true_col_valid)
        mean_pred = np.mean(pred_col_valid)
        var_true = np.var(true_col_valid)
        var_pred = np.var(pred_col_valid)
        cov_true_pred = np.mean((true_col_valid - mean_true) * (pred_col_valid - mean_pred))
        denominator_ccc = var_true + var_pred + (mean_true - mean_pred)**2
        ccc = (2 * cov_true_pred) / denominator_ccc if denominator_ccc != 0 else np.nan

        mse = mean_squared_error(true_col_valid, pred_col_valid)
        rmse = np.sqrt(mse)
        mae = mean_absolute_error(true_col_valid, pred_col_valid)

        metrics[col_name] = {
            'r': float(r), 'r2': float(r2), 'ccc': float(ccc),
            'rmse': float(rmse), 'mse': float(mse), 'mae': float(mae)
        }
    return metrics


def plot_roc_curve(y_true, scores, filename):
    """ç»˜åˆ¶å¸¦95%ç½®ä¿¡åŒºé—´çš„ROCæ›²çº¿å¹¶ä¿å­˜ã€‚"""
    plt.figure(figsize=(8, 8))
    tprs, aurocs = [], []
    base_fpr = np.linspace(0, 1, 101)

    # ä½¿ç”¨bootstrapè®¡ç®—ç½®ä¿¡åŒºé—´
    n_bootstrap = 2000
    for _ in range(n_bootstrap):
        indices = np.random.choice(len(y_true), len(y_true), replace=True)
        # ç¡®ä¿æŠ½æ ·æ•°æ®åŒ…å«è‡³å°‘ä¸¤ä¸ªç±»åˆ«ï¼Œå¦åˆ™è·³è¿‡
        if len(np.unique(y_true.astype(int)[indices])) < 2:
            continue
        fpr, tpr, _ = roc_curve(y_true.astype(int)[indices], scores.astype(float)[indices])
        tpr_interpolated = np.interp(base_fpr, fpr, tpr)
        tprs.append(tpr_interpolated)
        aurocs.append(auc(fpr, tpr))

    # è®¡ç®—å‡å€¼å’Œç½®ä¿¡åŒºé—´
    tprs = np.array(tprs)
    mean_tpr = np.mean(tprs, axis=0)
    std_tpr = np.std(tprs, axis=0)
    tprs_upper = np.minimum(mean_tpr + 1.96 * std_tpr, 1)
    tprs_lower = np.maximum(mean_tpr - 1.96 * std_tpr, 0)
    mean_auroc = np.mean(aurocs)

    plt.plot([0, 1], [0, 1], linestyle='--', color='grey', label='Chance')
    plt.plot(base_fpr, mean_tpr, 'b', label=f'Mean ROC (AUC = {mean_auroc:.2f})')
    auroc_lower = np.percentile(aurocs, 2.5)
    auroc_upper = np.percentile(aurocs, 97.5)
    plt.fill_between(base_fpr, tprs_lower, tprs_upper, color='lightblue', alpha=0.5, label=f'95% CI ({auroc_lower:.2f}-{auroc_upper:.2f})')

    plt.title('ROC Curve', fontsize=16)
    plt.xlabel('False Positive Rate', fontsize=12)
    plt.ylabel('True Positive Rate', fontsize=12)
    plt.legend(loc='lower right')
    plt.grid(True, linestyle='--')
    plt.tight_layout()
    plt.savefig(filename)
    plt.close()


def plot_roc_curve_multiclass(y_true, y_scores, n_classes, filename):
    """ç»˜åˆ¶å¸¦95%ç½®ä¿¡åŒºé—´çš„å¤šåˆ†ç±»ROCæ›²çº¿å¹¶ä¿å­˜ã€‚"""
    plt.figure(figsize=(10, 8))
    colors = ['b', 'r', 'g']
    class_names = ['COPD', 'NORMAL', 'PRISm']

    for i in range(n_classes):
        tprs, aurocs = [], []
        base_fpr = np.linspace(0, 1, 101)

        y_true_binary = (y_true == i).astype(int)

        # ä½¿ç”¨bootstrapè®¡ç®—ç½®ä¿¡åŒºé—´
        n_bootstrap = 2000
        for _ in range(n_bootstrap):
            indices = np.random.choice(len(y_true_binary), len(y_true_binary), replace=True)
            if len(np.unique(y_true_binary[indices])) < 2:
                continue
            fpr, tpr, _ = roc_curve(y_true_binary[indices], y_scores[indices, i])
            tpr_interpolated = np.interp(base_fpr, fpr, tpr)
            tprs.append(tpr_interpolated)
            aurocs.append(auc(fpr, tpr))

        # è®¡ç®—å‡å€¼å’Œç½®ä¿¡åŒºé—´
        tprs = np.array(tprs)
        mean_tpr = np.mean(tprs, axis=0)
        std_tpr = np.std(tprs, axis=0)
        tprs_upper = np.minimum(mean_tpr + 1.96 * std_tpr, 1)
        tprs_lower = np.maximum(mean_tpr - 1.96 * std_tpr, 0)
        mean_auroc = np.mean(aurocs)

        plt.plot(base_fpr, mean_tpr, color=colors[i], label=f'{class_names[i]} ROC (AUC = {mean_auroc:.2f})')
        # è®¡ç®—95% CI
        auroc_lower = np.percentile(aurocs, 2.5)
        auroc_upper = np.percentile(aurocs, 97.5)
        plt.fill_between(base_fpr, tprs_lower, tprs_upper, color=colors[i], alpha=0.15, label=f'95% CI ({auroc_lower:.2f}-{auroc_upper:.2f})')

    plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='grey', label='Chance')
    plt.title('Multi-Class ROC Curve', fontsize=16)
    plt.xlabel('False Positive Rate', fontsize=12)
    plt.ylabel('True Positive Rate', fontsize=12)
    plt.legend(loc='lower right')
    plt.grid(True, linestyle='--')
    plt.tight_layout()
    plt.savefig(filename)
    plt.close()


def plot_bland_altman(true_vals, pred_vals, title, filename):
    """ç»˜åˆ¶Bland-Altmanå›¾å¹¶ä¿å­˜ã€‚"""
    plt.figure(figsize=(10, 8))

    diff = pred_vals - true_vals
    mean_diff = np.mean(diff)
    std_diff = np.std(diff)
    loa_upper = mean_diff + 1.96 * std_diff
    loa_lower = mean_diff - 1.96 * std_diff

    mean_vals = (true_vals + pred_vals) / 2

    plt.scatter(mean_vals, diff, alpha=0.5, s=20)
    plt.axhline(mean_diff, color='gray', linestyle='--', label=f'Mean Difference: {mean_diff:.2f}')
    plt.axhline(loa_upper, color='red', linestyle='--', label=f'95% LoA: {loa_upper:.2f}')
    plt.axhline(loa_lower, color='red', linestyle='--')

    plt.title(title, fontsize=16)
    plt.xlabel('Mean of Measurements', fontsize=12)
    plt.ylabel('Difference (Prediction - True)', fontsize=12)
    plt.legend()
    plt.grid(True, linestyle='--')
    plt.tight_layout()
    plt.savefig(filename)
    plt.close()


# --- æ•°æ®åŠ è½½ä¸å‡†å¤‡ ---
def load_and_prepare_test_data(data_root_path, csv_file_path, csv_input_scaler, one_hot_cols):
    """åŠ è½½æµ‹è¯•é›†çš„å½±åƒå’ŒCSVæ•°æ®ã€‚"""
    try:
        current_df = pd.read_csv(csv_file_path, index_col='åºå·')
    except FileNotFoundError:
        print(f"é”™è¯¯: æœªæ‰¾åˆ°æµ‹è¯•é›† CSV æ–‡ä»¶ '{csv_file_path}'ã€‚è¯·æ£€æŸ¥è·¯å¾„ã€‚")
        return [], None, None

    data_list = []
    search_pattern = os.path.join(data_root_path, "*.nii.gz")
    all_nii_files = glob.glob(search_pattern)
    image_files = {int(os.path.basename(f).split('.')[0]): f for f in all_nii_files if '_mask' not in f}
    mask_files = {int(os.path.basename(f).split('_')[0]): f for f in all_nii_files if '_mask' in f}

    for numeric_id in sorted(current_df.index.tolist()):
        image_path = image_files.get(numeric_id)
        mask_path = mask_files.get(numeric_id)
        if image_path and mask_path:
            if not current_df.loc[numeric_id, TARGET_COLUMNS].isnull().any():
                data_list.append({
                    "id": str(numeric_id),
                    "image": image_path,
                    "mask": mask_path,
                    "csv_data": current_df.loc[numeric_id]
                })

    if not data_list:
        print("é”™è¯¯: æ‰¾ä¸åˆ°ä»»ä½•åŒ¹é…çš„å½±åƒã€æ©ç å’ŒCSVæ•°æ®ã€‚")
        return [], None, None

    raw_test_df = pd.DataFrame([d['csv_data'] for d in data_list], index=[d['id'] for d in data_list])
    preprocessed_df = pd.get_dummies(raw_test_df, columns=ONE_HOT_INPUT_FEATURES, prefix=ONE_HOT_INPUT_FEATURES, dummy_na=False)
    missing_cols = set(one_hot_cols) - set(preprocessed_df.columns)
    for c in missing_cols:
        preprocessed_df[c] = 0
    preprocessed_df = preprocessed_df[one_hot_cols + list(preprocessed_df.columns.drop(one_hot_cols))]
    final_csv_input_features = NORMAL_CSV_INPUT_FEATURES + one_hot_cols
    final_csv_input_features = [col for col in final_csv_input_features if col in preprocessed_df.columns]
    preprocessed_df[final_csv_input_features] = csv_input_scaler.transform(preprocessed_df[final_csv_input_features])

    test_files = []
    id_to_data = {d['id']: d for d in data_list}
    for idx in raw_test_df.index:
        raw_info = id_to_data[str(idx)]
        test_files.append({
            "image": raw_info['image'],
            "mask": raw_info['mask'],
            "csv_input_features": preprocessed_df.loc[str(idx), final_csv_input_features].values.astype(np.float32),
            "targets": raw_test_df.loc[idx, TARGET_COLUMNS].values.astype(np.float32),
            "original_type": raw_test_df.loc[idx, 'ç±»å‹'],
            "sex": raw_test_df.loc[idx, 'æ€§åˆ«'],
            "age": raw_test_df.loc[idx, 'å¹´é¾„'],
            "height": raw_test_df.loc[idx, 'èº«é«˜'],
            "id": str(idx)
        })

    return test_files, final_csv_input_features, raw_test_df


# --- ä¸»æ‰§è¡Œå‡½æ•° ---
def predict_and_evaluate():
    if not os.path.exists(MODEL_OUTPUT_DIR):
        print(f"é”™è¯¯: æ‰¾ä¸åˆ°æ¨¡å‹ç›®å½• '{MODEL_OUTPUT_DIR}'ã€‚")
        return

    if not os.path.exists(REPORT_OUTPUT_DIR):
        os.makedirs(REPORT_OUTPUT_DIR)
        print(f"å·²åˆ›å»ºæŠ¥å‘Šç›®å½•: {REPORT_OUTPUT_DIR}")

    # --- 1. åŠ è½½æ ‡å‡†åŒ–å™¨ ---
    try:
        with open(os.path.join(MODEL_OUTPUT_DIR, "input_scaler.pkl"), 'rb') as f:
            csv_input_scaler = pickle.load(f)
        with open(os.path.join(MODEL_OUTPUT_DIR, "target_scaler.pkl"), 'rb') as f:
            target_scaler = pickle.load(f)
        print("æ ‡å‡†åŒ–å™¨åŠ è½½æˆåŠŸã€‚")
        one_hot_cols = [col for col in csv_input_scaler.feature_names_in_ if any(f in col for f in ONE_HOT_INPUT_FEATURES)]
    except FileNotFoundError:
        print("é”™è¯¯: æ‰¾ä¸åˆ°å¿…è¦çš„æ ‡å‡†åŒ–å™¨æ–‡ä»¶ã€‚")
        return

    # --- 2. åŠ è½½å’Œå‡†å¤‡æµ‹è¯•æ•°æ® ---
    test_files, all_csv_features_list, raw_test_df = load_and_prepare_test_data(DATA_ROOT_PATH, TEST_CSV_FILE_PATH, csv_input_scaler, one_hot_cols)
    if not test_files:
        print("æ— æ³•è¿›è¡Œé¢„æµ‹ï¼Œè¯·æ£€æŸ¥æ•°æ®åŠ è½½é”™è¯¯ä¿¡æ¯ã€‚")
        return

    num_csv_features = len(all_csv_features_list)
    model_to_load = model.CrossModalNet(
        image_network_name=image_network_name,
        num_image_channels=1,
        num_csv_input_features=num_csv_features,
        num_targets=len(TARGET_COLUMNS)
    ).to(DEVICE)

    model_path = os.path.join(MODEL_OUTPUT_DIR, "best_model.pth")
    if not os.path.exists(model_path):
        print(f"é”™è¯¯: æ‰¾ä¸åˆ°æ¨¡å‹æƒé‡æ–‡ä»¶ '{model_path}'ã€‚")
        return

    model_to_load.load_state_dict(torch.load(model_path))
    model_to_load.eval()
    print("æ¨¡å‹åŠ è½½æˆåŠŸã€‚")

    transforms = Compose(
        [
            LoadImaged(keys=["image", "mask"]),
            EnsureChannelFirstd(keys=["image", "mask"]),
            ScaleIntensityRanged(
                keys=["image"],
                a_min=HU_LOWER_BOUND,
                a_max=HU_UPPER_BOUND,
                b_min=0.0,
                b_max=1.0,
                clip=True,
            ),
            Orientationd(keys=["image", "mask"], axcodes="RAS"),
            Spacingd(keys=["image", "mask"], pixdim=TARGET_SPACING, mode=("bilinear", "nearest")),
            CropForegroundd(keys=["image", "mask"], source_key="mask"),
            Resized(
                keys=["image", "mask"],
                spatial_size=TARGET_SIZE,
                mode=("trilinear", "nearest")
            ),
            MaskIntensityd(keys="image", mask_key="mask"),
            ToTensord(keys=["image", "targets", "csv_input_features"]),
        ]
    )

    # --- 3. åˆ›å»º DataLoader ---
    test_ds = PersistentDataset(data=test_files, transform=transforms, cache_dir=os.path.join(persistent_cache, "test_predict"))
    test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=torch.cuda.is_available())

    # --- 4. è¿›è¡Œé¢„æµ‹ ---
    print("\n--- å¼€å§‹åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œé¢„æµ‹ ---")
    all_preds_unscaled = []
    all_targets_unscaled = []
    all_fev1_fvc_ratios = []
    # ç¡®ä¿ä½¿ç”¨åŸå§‹çš„raw_test_dfæ¥è·å–éå¤„ç†è¿‡çš„åˆ†ç±»ä¿¡æ¯
    all_raw_data_df = raw_test_df.loc[raw_test_df.index.astype(str).isin([d['id'] for d in test_files])].reset_index(drop=True)

    with torch.no_grad():
        for batch_data in tqdm(test_loader, desc="å¤„ç†æµ‹è¯•é›†"):
            inputs, targets_unscaled_batch, csv_input_features = (
                batch_data["image"].to(DEVICE),
                batch_data["targets"].to(DEVICE),
                batch_data["csv_input_features"].to(DEVICE)
            )
            outputs_scaled,image_features = model_to_load(inputs, csv_input_features)

            y_pred_unscaled = target_scaler.inverse_transform(outputs_scaled.detach().cpu().numpy())
            y_true_unscaled = targets_unscaled_batch.detach().cpu().numpy()

            all_preds_unscaled.append(y_pred_unscaled)
            all_targets_unscaled.append(y_true_unscaled)

            # è®¡ç®—æ¨¡å‹é¢„æµ‹çš„ FEV1/FVC æ¯”å€¼
            fev1_pred = y_pred_unscaled[:, 1]
            fvc_pred = y_pred_unscaled[:, 0]
            fev1_fvc_ratio = (fev1_pred / fvc_pred) * 100
            all_fev1_fvc_ratios.extend(fev1_fvc_ratio)

    all_preds_unscaled = np.concatenate(all_preds_unscaled, axis=0)
    all_targets_unscaled = np.concatenate(all_targets_unscaled, axis=0)
    all_fev1_fvc_ratios = np.array(all_fev1_fvc_ratios)

    # --- 5. ä¿å­˜å›å½’æŒ‡æ ‡åˆ°txt ---
    print("\n--- ä¿å­˜å›å½’æŒ‡æ ‡åˆ°æ–‡ä»¶ ---")
    metrics = calculate_regression_metrics(all_targets_unscaled, all_preds_unscaled, TARGET_COLUMNS)
    report_path = os.path.join(REPORT_OUTPUT_DIR, "metrics_report.txt")
    with open(report_path, "w") as f:
        f.write("--- Regression Metrics on Test Set ---\n\n")
        for col in TARGET_COLUMNS:
            if col in metrics:
                m = metrics[col]
                f.write(f"  {col}:\n")
                f.write(f"    r (Pearson Correlation): {m['r']:.4f}\n")
                f.write(f"    r2 (Coefficient of Determination): {m['r2']:.4f}\n")
                f.write(f"    ccc (Concordance Correlation Coefficient): {m['ccc']:.4f}\n")
                f.write(f"    rmse (Root Mean Squared Error): {m['rmse']:.4f}\n")
                f.write(f"    mse (Mean Squared Error): {m['mse']:.4f}\n")
                f.write(f"    mae (Mean Absolute Error): {m['mae']:.4f}\n")
                f.write("\n")
    print(f"å›å½’æŒ‡æ ‡å·²ä¿å­˜è‡³: {report_path}")

    # --- 6. COPDäºŒåˆ†ç±»å’ŒæŠ¥å‘Š ---
    print("\n--- æ‰§è¡ŒCOPDäºŒåˆ†ç±»å’ŒæŠ¥å‘Š ---")
    y_true_types_2class = all_raw_data_df['ç±»å‹'].fillna('UNKNOWN').values
    y_true_binary = np.where(y_true_types_2class == 'COPD', 1, 0)
    y_pred_binary = np.where(all_fev1_fvc_ratios < 70, 1, 0)

    # åˆ†ç±»æŠ¥å‘Š
    class_report_2class = classification_report(y_true_binary, y_pred_binary, target_names=['Non-COPD', 'COPD'], zero_division=0)
    report_path_2class = os.path.join(REPORT_OUTPUT_DIR, "2classify_classification_report.txt")
    with open(report_path_2class, "w") as f:
        f.write("--- 2-Class Classification Report ---\n\n")
        f.write(class_report_2class)
    print(f"äºŒåˆ†ç±»æŠ¥å‘Šå·²ä¿å­˜è‡³: {report_path_2class}")

    # ç»˜åˆ¶äºŒåˆ†ç±»æ··æ·†çŸ©é˜µ
    unique_numerical_labels_2class = np.unique(y_true_binary)
    label_names_map_2class = {0: 'Non-COPD', 1: 'COPD'}
    existing_labels_2class = [label_names_map_2class[i] for i in unique_numerical_labels_2class]

    cm_2class = confusion_matrix(y_true_binary, y_pred_binary, labels=unique_numerical_labels_2class)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm_2class, annot=True, fmt='d', cmap='Blues', xticklabels=existing_labels_2class, yticklabels=existing_labels_2class)
    plt.title('2-Class Confusion Matrix', fontsize=16)
    plt.ylabel('True Label', fontsize=12)
    plt.xlabel('Predicted Label', fontsize=12)
    plt.tight_layout()
    plt.savefig(os.path.join(REPORT_OUTPUT_DIR, "2classify_confusion_matrix.png"))
    plt.close()

    # ç»˜åˆ¶äºŒåˆ†ç±»ROCæ›²çº¿
    reversed_scores = 100 - all_fev1_fvc_ratios
    plot_roc_curve(y_true_binary, reversed_scores, os.path.join(REPORT_OUTPUT_DIR, "2classify_roc_curve.png"))

    print("äºŒåˆ†ç±»å›¾è¡¨å·²ä¿å­˜ã€‚")


    # --- 7. ä¸‰åˆ†ç±»åˆ†æå’ŒæŠ¥å‘Š ---
    print("\n--- æ‰§è¡Œä¸‰åˆ†ç±»åˆ†æå’ŒæŠ¥å‘Š ---")
    # ğŸš¨ ä¿®æ”¹ç‚¹: ç¡®ä¿ä½¿ç”¨ 'åˆ†ç±»' åˆ—ä½œä¸ºçœŸå®æ ‡ç­¾ ğŸš¨
    y_true_3class = all_raw_data_df['åˆ†ç±»'].fillna('UNKNOWN').values
    y_pred_3class = np.array([
        predict_three_classes(
            all_preds_unscaled[i, 1],
            all_preds_unscaled[i, 0],
            all_raw_data_df.loc[i, 'å¹´é¾„'],
            all_raw_data_df.loc[i, 'èº«é«˜'],
            all_raw_data_df.loc[i, 'æ€§åˆ«']
        )
        for i in range(len(all_preds_unscaled))
    ])

    # å‰”é™¤æ— æ³•åˆ†ç±»çš„æ ·æœ¬
    valid_indices = (y_true_3class != 'UNKNOWN') & (y_pred_3class != 'UNKNOWN')
    y_true_3class_filtered = y_true_3class[valid_indices]
    y_pred_3class_filtered = y_pred_3class[valid_indices]

    # ç¡®ä¿åœ¨è®¡ç®—å‰ï¼Œæœ‰æ•ˆæ ·æœ¬ä¸ä¸ºç©º
    if len(y_true_3class_filtered) == 0:
        print("è­¦å‘Š: ä¸‰åˆ†ç±»åˆ†æçš„æœ‰æ•ˆæ ·æœ¬æ•°é‡ä¸º0ï¼Œæ— æ³•ç”ŸæˆæŠ¥å‘Šå’Œå›¾è¡¨ã€‚")
        return

    # åˆ†ç±»æŠ¥å‘Š
    class_names_3class = ['COPD', 'NORMAL', 'PRISm']
    class_report_3class = classification_report(
        y_true_3class_filtered,
        y_pred_3class_filtered,
        labels=class_names_3class,
        zero_division=0
    )
    report_path_3class = os.path.join(REPORT_OUTPUT_DIR, "3classify_classification_report.txt")
    with open(report_path_3class, "w") as f:
        f.write("--- 3-Class Classification Report ---\n\n")
        f.write(class_report_3class)
    print(f"ä¸‰åˆ†ç±»æŠ¥å‘Šå·²ä¿å­˜è‡³: {report_path_3class}")

    # ç»˜åˆ¶ä¸‰åˆ†ç±»æ··æ·†çŸ©é˜µ
    cm_3class = confusion_matrix(y_true_3class_filtered, y_pred_3class_filtered, labels=class_names_3class)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm_3class, annot=True, fmt='d', cmap='Blues', xticklabels=class_names_3class, yticklabels=class_names_3class)
    plt.title('3-Class Confusion Matrix', fontsize=16)
    plt.ylabel('True Label', fontsize=12)
    plt.xlabel('Predicted Label', fontsize=12)
    plt.tight_layout()
    plt.savefig(os.path.join(REPORT_OUTPUT_DIR, "3classify_confusion_matrix.png"))
    plt.close()

    # ç»˜åˆ¶ä¸‰åˆ†ç±»ROCæ›²çº¿ (éœ€è¦æ¦‚ç‡é¢„æµ‹ï¼Œè¿™é‡Œç”¨é¢„æµ‹æ¯”å€¼ä½œä¸ºè¿‘ä¼¼)
    # åˆ¶ä½œä¸‰åˆ†ç±»çš„æ•°å€¼æ ‡ç­¾
    y_true_3class_numerical = np.array([class_names_3class.index(c) for c in y_true_3class_filtered])

    # æ„å»ºä¼ªæ¦‚ç‡çŸ©é˜µï¼Œç”¨äºROCæ›²çº¿ç»˜åˆ¶
    filtered_fev1_fvc_ratios = all_fev1_fvc_ratios[valid_indices]

    y_scores_3class = np.zeros((len(filtered_fev1_fvc_ratios), 3))

    # COPD: FEV1/FVC < 70, score is inverse of ratio
    y_scores_3class[:, 0] = 100 - filtered_fev1_fvc_ratios

    # NORMAL & PRISm: éœ€è¦é‡æ–°è®¡ç®—ï¼Œå› ä¸ºå®ƒä»¬çš„æŒ‡æ ‡ä¾èµ–äºç™¾åˆ†æ¯”
    fev1_percents_filtered = np.zeros_like(filtered_fev1_fvc_ratios)
    fvc_percents_filtered = np.zeros_like(filtered_fev1_fvc_ratios)

    raw_data_filtered = all_raw_data_df.loc[valid_indices].reset_index(drop=True)
    preds_unscaled_filtered = all_preds_unscaled[valid_indices]

    for i in range(len(filtered_fev1_fvc_ratios)):
        raw_data = raw_data_filtered.loc[i]
        predicted_fev1 = preds_unscaled_filtered[i, 1]
        predicted_fvc = preds_unscaled_filtered[i, 0]

        if raw_data['æ€§åˆ«'] == 'ç”·':
            fev1_pred_calc = calculate_fev1_pred_male(raw_data['å¹´é¾„'], raw_data['èº«é«˜'])
            fvc_pred_calc = calculate_fvc_pred_male(raw_data['å¹´é¾„'], raw_data['èº«é«˜'])
        else:
            fev1_pred_calc = calculate_fev1_pred_female(raw_data['å¹´é¾„'], raw_data['èº«é«˜'])
            fvc_pred_calc = calculate_fvc_pred_female(raw_data['å¹´é¾„'], raw_data['èº«é«˜'])

        fev1_percents_filtered[i] = (predicted_fev1 / fev1_pred_calc) * 100
        fvc_percents_filtered[i] = (predicted_fvc / fvc_pred_calc) * 100

    y_scores_3class[:, 1] = fev1_percents_filtered + fvc_percents_filtered
    y_scores_3class[:, 2] = 200 - (fev1_percents_filtered + fvc_percents_filtered)

    plot_roc_curve_multiclass(y_true_3class_numerical, y_scores_3class, 3, os.path.join(REPORT_OUTPUT_DIR, "3classify_roc_curve.png"))

    print("ä¸‰åˆ†ç±»å›¾è¡¨å·²ä¿å­˜ã€‚")

    # Bland-Altmanå›¾
    print("\n--- ç»˜åˆ¶å¹¶ä¿å­˜å›å½’å›¾è¡¨ ---")
    # FVC
    plot_bland_altman(all_targets_unscaled[:, 0], all_preds_unscaled[:, 0],
                      'Bland-Altman Plot for FVC', os.path.join(REPORT_OUTPUT_DIR, "bland_altman_fvc.png"))
    # FEV1
    plot_bland_altman(all_targets_unscaled[:, 1], all_preds_unscaled[:, 1],
                      'Bland-Altman Plot for FEV1', os.path.join(REPORT_OUTPUT_DIR, "bland_altman_fev1.png"))

    print("æ‰€æœ‰å›¾è¡¨å·²ä¿å­˜ã€‚")

# --- ä¿®æ­£åçš„ç»˜å›¾å‡½æ•° ---
def plot_training_metrics():
    """æ ¹æ® training_metrics_history.json ç»˜åˆ¶è®­ç»ƒå’ŒéªŒè¯æŒ‡æ ‡æ›²çº¿å›¾ã€‚"""
    # ä¿®æ­£ç‚¹ï¼šç›´æ¥åœ¨æ¨¡å‹è¾“å‡ºç›®å½•å¯»æ‰¾JSONæ–‡ä»¶
    metrics_file_path = os.path.join(MODEL_OUTPUT_DIR, 'training_metrics_history.json')

    if not os.path.exists(metrics_file_path):
        print(f"\nè­¦å‘Š: æ‰¾ä¸åˆ°è®­ç»ƒæŒ‡æ ‡æ–‡ä»¶ '{metrics_file_path}'ï¼Œè·³è¿‡ç»˜åˆ¶è®­ç»ƒæ›²çº¿ã€‚")
        return

    try:
        with open(metrics_file_path, 'r') as f:
            data = json.load(f)
    except json.JSONDecodeError:
        print(f"\né”™è¯¯: æ–‡ä»¶ '{metrics_file_path}' æ ¼å¼ä¸æ­£ç¡®ï¼Œæ— æ³•è§£æã€‚")
        return

    train_loss_history = data.get('train_loss_history', [])
    val_metrics_history = data.get('val_metrics_history', [])

    if not train_loss_history and not val_metrics_history:
        print("\nè­¦å‘Š: JSON æ–‡ä»¶ä¸­æ²¡æœ‰è®­ç»ƒæˆ–éªŒè¯æ•°æ®ã€‚")
        return

    # ä¿®æ­£ç‚¹ï¼šä» FVC å’Œ FEV1 çš„ mse ä¸­è®¡ç®—å¹³å‡éªŒè¯æŸå¤±
    val_loss_history = [
        (d['FVC']['mse'] + d['FEV1']['mse']) / 2
        for d in val_metrics_history
    ]

    metrics_to_plot = ['r', 'r2', 'rmse', 'mse', 'mae', 'ccc']
    val_FVC_metrics = {metric: [d['FVC'][metric] for d in val_metrics_history] for metric in metrics_to_plot}
    val_FEV1_metrics = {metric: [d['FEV1'][metric] for d in val_metrics_history] for metric in metrics_to_plot}

    epochs_train = range(1, len(train_loss_history) + 1)
    epochs_val = range(1, len(val_loss_history) + 1)

    # 1. ç»˜åˆ¶æŸå¤±æ›²çº¿
    plt.figure(figsize=(10, 6))
    plt.plot(epochs_train, train_loss_history, label='Train Loss')
    if val_loss_history:
        plt.plot(epochs_val, val_loss_history, label='Validation Loss')
    plt.title('Training and Validation Loss', fontsize=16)
    plt.xlabel('Epoch', fontsize=12)
    plt.ylabel('Loss', fontsize=12)
    plt.legend()
    plt.grid(True, linestyle='--')
    plt.tight_layout()
    plt.savefig(os.path.join(REPORT_OUTPUT_DIR, "loss_history.png"))
    plt.close()
    print("\næŸå¤±æ›²çº¿å›¾å·²ä¿å­˜è‡³: loss_history.png")

    # 2. ç»˜åˆ¶ FVC ç›®æ ‡çš„å…­å®«æ ¼æŒ‡æ ‡æ›²çº¿
    fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(18, 12))
    fig.suptitle('FVC Validation Metrics History', fontsize=20)
    axes = axes.flatten()

    for i, metric in enumerate(metrics_to_plot):
        axes[i].plot(epochs_val, val_FVC_metrics[metric], label='FVC ' + metric, )
        axes[i].set_title(metric, fontsize=14)
        axes[i].set_xlabel('Epoch', fontsize=10)
        axes[i].set_ylabel(metric, fontsize=10)
        axes[i].grid(True, linestyle='--')
        axes[i].legend()

    plt.tight_layout(rect=[0, 0.03, 1, 0.95])
    plt.savefig(os.path.join(REPORT_OUTPUT_DIR, "fvc_metrics_history.png"))
    plt.close()
    print("FVCæŒ‡æ ‡æ›²çº¿å›¾å·²ä¿å­˜è‡³: fvc_metrics_history.png")

    # 3. ç»˜åˆ¶ FEV1 ç›®æ ‡çš„å…­å®«æ ¼æŒ‡æ ‡æ›²çº¿
    fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(18, 12))
    fig.suptitle('FEV1 Validation Metrics History', fontsize=20)
    axes = axes.flatten()

    for i, metric in enumerate(metrics_to_plot):
        axes[i].plot(epochs_val, val_FEV1_metrics[metric], label='FEV1 ' + metric, )
        axes[i].set_title(metric, fontsize=14)
        axes[i].set_xlabel('Epoch', fontsize=10)
        axes[i].set_ylabel(metric, fontsize=10)
        axes[i].grid(True, linestyle='--')
        axes[i].legend()

    plt.tight_layout(rect=[0, 0.03, 1, 0.95])
    plt.savefig(os.path.join(REPORT_OUTPUT_DIR, "fev1_metrics_history.png"))
    plt.close()
    print("FEV1æŒ‡æ ‡æ›²çº¿å›¾å·²ä¿å­˜è‡³: fev1_metrics_history.png")

if __name__ == "__main__":
    predict_and_evaluate()
    # ğŸš¨ å†æ¬¡è°ƒç”¨ä¿®æ­£åçš„ç»˜å›¾å‡½æ•° ğŸš¨
    plot_training_metrics()